{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Transfer Learning From Pre-Trained Model for Face Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayank-goyal2424/Deep_learning/blob/master/Transfer_Learning_From_Pre_Trained_Model_for_Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu36rAFNVB1J",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning From Pre-Trained Model (VGG16)  for Face Recognition\n",
        "\n",
        "### Loading the VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uslcvyIVB1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes but we are setting image size 100x100 to reduce computing power\n",
        "img_rows = 100\n",
        "img_cols = 100\n",
        "\n",
        "#Loads the VGG16 model \n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBLMQPnVVB1o",
        "colab_type": "text"
      },
      "source": [
        "### Inpsecting each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by8GQCZ7VB1r",
        "colab_type": "code",
        "colab": {},
        "outputId": "15de9be0-2a40-46d2-a5cf-eee3c67b0a27"
      },
      "source": [
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D True\n",
            "2 Conv2D True\n",
            "3 MaxPooling2D True\n",
            "4 Conv2D True\n",
            "5 Conv2D True\n",
            "6 MaxPooling2D True\n",
            "7 Conv2D True\n",
            "8 Conv2D True\n",
            "9 Conv2D True\n",
            "10 MaxPooling2D True\n",
            "11 Conv2D True\n",
            "12 Conv2D True\n",
            "13 Conv2D True\n",
            "14 MaxPooling2D True\n",
            "15 Conv2D True\n",
            "16 Conv2D True\n",
            "17 Conv2D True\n",
            "18 MaxPooling2D True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqOdNsDVB19",
        "colab_type": "text"
      },
      "source": [
        "### Let's freeze all layers except the top 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O43n8trNVB2A",
        "colab_type": "code",
        "colab": {},
        "outputId": "24447ec8-f284-4e95-9ef1-a4278ef68239"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes but we are setting image size 100x100 to reduce computing power\n",
        "img_rows = 100\n",
        "img_cols = 100 \n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Layers are set to trainable as True by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpDiNUrdVB2M",
        "colab_type": "text"
      },
      "source": [
        "### Let's make a function that returns our FC Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_8V_QFVB2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addTopModel(bottom_model, num_classes, D=256):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "    top_model = bottom_model.output\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\n",
        "    top_model = Dense(512,activation='relu')(top_model)\n",
        "    top_model = Dense(256,activation='relu')(top_model)\n",
        "    top_model = Dropout(0.3)(top_model)\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
        "    return top_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJFODgWkVB2e",
        "colab_type": "code",
        "colab": {},
        "outputId": "28e6d3f4-ab67-437b-a589-56d6e1de3a7c"
      },
      "source": [
        "model.input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_4:0' shape=(None, 100, 100, 3) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4B3ShfmVB2w",
        "colab_type": "code",
        "colab": {},
        "outputId": "8feacd91-8329-437a-8b74-32710d63a58a"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x6b94fb2448>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fb25c8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fb2908>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x6b94fb0848>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fb5488>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fb8f88>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x6b94fbaac8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fbd308>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fbfdc8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fc3ac8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x6b94fc97c8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fcd408>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b94fcfec8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b95102a48>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x6b951078c8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b9510b508>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b95112348>,\n",
              " <keras.layers.convolutional.Conv2D at 0x6b95112c88>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x6b951179c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-e4Wf_OVB2_",
        "colab_type": "text"
      },
      "source": [
        "### Let's add our FC Head back onto VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYMxe_chVB3C",
        "colab_type": "code",
        "colab": {},
        "outputId": "a16fb762-afd7-408d-e5cb-46556bf9f84b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "FC_Head = addTopModel(model, num_classes)\n",
        "\n",
        "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
        "\n",
        "print(modelnew.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 100, 100, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               2359808   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 17,208,394\n",
            "Trainable params: 2,493,706\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_7mB5sSVB3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKR_qn02VB3a",
        "colab_type": "text"
      },
      "source": [
        "### Loading our Bollywood  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55NMdd-eVB3b",
        "colab_type": "code",
        "colab": {},
        "outputId": "7b3036fc-f33b-43d9-b5d0-8ef4893cda4f"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = 'Bollywood_Dataset/Bollywood_Dataset/train/'\n",
        "validation_data_dir = 'Bollywood_Dataset/Bollywood_Dataset/validation/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 15\n",
        "val_batchsize = 10\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 300 images belonging to 10 classes.\n",
            "Found 100 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPHiJw9MVB3k",
        "colab_type": "text"
      },
      "source": [
        "### Training our top layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WDAqp6wVB3m",
        "colab_type": "code",
        "colab": {},
        "outputId": "d891671b-2c6e-413b-9b41-2e0cf5b49906"
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"Bollywood_Dataset.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "modelnew.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 300\n",
        "nb_validation_samples = 100\n",
        "epochs = 10\n",
        "batch_size = 15\n",
        "\n",
        "classifier = modelnew.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "modelnew.save(\"Bollywood_Dataset.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 36s 2s/step - loss: 2.9188 - accuracy: 0.1300 - val_loss: 2.3599 - val_accuracy: 0.0333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.35990, saving model to Bollywood_Dataset.h5\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 36s 2s/step - loss: 2.0674 - accuracy: 0.2300 - val_loss: 2.3458 - val_accuracy: 0.1500\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.35990 to 2.34582, saving model to Bollywood_Dataset.h5\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 32s 2s/step - loss: 1.8106 - accuracy: 0.3667 - val_loss: 1.9203 - val_accuracy: 0.1500\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.34582 to 1.92034, saving model to Bollywood_Dataset.h5\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 30s 1s/step - loss: 1.6720 - accuracy: 0.4267 - val_loss: 2.0027 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.92034\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 30s 2s/step - loss: 1.4455 - accuracy: 0.4933 - val_loss: 0.9232 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.92034 to 0.92315, saving model to Bollywood_Dataset.h5\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 32s 2s/step - loss: 1.3620 - accuracy: 0.5333 - val_loss: 1.5105 - val_accuracy: 0.4333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.92315\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 31s 2s/step - loss: 1.2326 - accuracy: 0.5867 - val_loss: 2.7862 - val_accuracy: 0.5167\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.92315\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 31s 2s/step - loss: 1.1413 - accuracy: 0.5633 - val_loss: 1.3757 - val_accuracy: 0.5000\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.92315\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VYMWzJpVB3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading saved model for prediction\n",
        "\n",
        "from keras.models import load_model\n",
        "classifier = load_model('Bollywood_Dataset.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0isUIct2VB3-",
        "colab_type": "code",
        "colab": {},
        "outputId": "658d9f43-34ce-4630-9285-634383591219"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "actors_dataset_dict = {\"[0]\": \"Aamir Khan\", \n",
        "                      \"[1]\": \"Akshay Kumar\",\n",
        "                      \"[2]\": \"Hrithik Roshan\",\n",
        "                      \"[3]\": \"Kajal Agarwal\",\n",
        "                      \"[4]\": \"Kareena Kapoor \",\n",
        "                      \"[5]\": \"Katrina Kaif\", \n",
        "                      \"[6]\": \"Madhuri Dixit\",\n",
        "                      \"[7]\": \"Salman Khan\",\n",
        "                      \"[8]\": \"ShahRukh Khan\",\n",
        "                      \"[9]\": \"Shilpa Shetty \"\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "def draw_test(name, pred, im):\n",
        "    actors = actors_dataset_dict[str(pred)]\n",
        "    BLACK = [0,0,0]\n",
        "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
        "    cv2.putText(expanded_image, actors, (0, 40) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
        "    cv2.imshow(name, expanded_image)\n",
        "\n",
        "def getRandomImage(path):\n",
        "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
        "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
        "    random_directory = np.random.randint(0,len(folders))\n",
        "    path_class = folders[random_directory]\n",
        "    print(\"Class - \" + str(path_class))\n",
        "    file_path = path + path_class\n",
        "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
        "    random_file_index = np.random.randint(0,len(file_names))\n",
        "    image_name = file_names[random_file_index]\n",
        "    return cv2.imread(file_path+\"/\"+image_name)    \n",
        "\n",
        "\n",
        "input_im = getRandomImage(\"Bollywood_Dataset/Bollywood_Dataset/validation/\")\n",
        "input_original = input_im.copy()\n",
        "input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
        "    \n",
        "input_im = cv2.resize(input_im, (100, 100), interpolation = cv2.INTER_LINEAR)\n",
        "input_im = input_im / 255.\n",
        "input_im = input_im.reshape(1,100,100,3) \n",
        "    \n",
        "    # Get Prediction\n",
        "\n",
        "res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1)\n",
        "    \n",
        "    # Show image with predicted class\n",
        "draw_test(\"Prediction\", res, input_original) \n",
        "cv2.waitKey(5000)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class - Akshay Kumar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu1LnIiNVB4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YuJXcr8VB4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}