{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Neural Network Handson.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayank-goyal2424/Deep_learning/blob/master/Copy_of_Neural_Network_Handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xboM7ceKcBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu2HVD7TKcBQ",
        "colab_type": "text"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/workshops/Neural_Networks/Idiomatic Programmer - handbook 1 - Codelab 1.ipynb\">\n",
        "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAdlKfsoKcBR",
        "colab_type": "text"
      },
      "source": [
        "# Idiomatic Programmer - Code Labs \n",
        "\n",
        "## Code Lab #1 - Get Started with a Deep Neural Network (DNN)\n",
        "\n",
        "## Prerequistes:\n",
        "\n",
        "    1. Familiar with Python\n",
        "    2. Completed Handbook 1/Part 1: Neural Networks\n",
        "    \n",
        "## Objectives:\n",
        "\n",
        "    1. Install Keras related packages\n",
        "    2. Create a basic DNN (input layer, hidden layer, output layer)\n",
        "        A. Correctly set the input shape.\n",
        "        B. Correctly set the number of nodes.\n",
        "        C. Correctly set activation function.\n",
        "        D. Practive using various Sequential API styles and the Functional API.\n",
        "    3. Review a model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktDZQ91uKcBT",
        "colab_type": "text"
      },
      "source": [
        "## Setup:\n",
        "\n",
        "Install relevant packages to get started with Keras, and then import them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awDNfEv-KcBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc3f09ae-784f-4bbb-eee4-3f245a94e785"
      },
      "source": [
        "# Install numpy math library\n",
        "!pip install -U numpy\n",
        "# Install Keras framework\n",
        "!pip install -U keras\n",
        "# Install Tensorflow backend\n",
        "!pip install -U tensorflow\n",
        "\n",
        "# Import keras and TF\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(np.__version__)              # should be 1.16.4\n",
        "print(tf.__version__)              # should be 1.13.1\n",
        "print(keras.__version__)           # should be 2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 216kB/s \n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "Successfully installed numpy-1.19.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.19.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 158kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.1\n",
            "    Uninstalling numpy-1.19.1:\n",
            "      Successfully uninstalled numpy-1.19.1\n",
            "Successfully installed numpy-1.18.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.18.5\n",
            "2.3.0\n",
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvmIL_GNKcBX",
        "colab_type": "text"
      },
      "source": [
        "## Basic DNN as Sequential API (long form)\n",
        "\n",
        "Let's use the long form of a Sequential method. That means that we will use the add() method for each step.\n",
        "\n",
        "Below is a nearly complete DNN for a multi-class classifier. You fill in the blanks (replace the ??), make sure it passes the Python interpreter, and then verify it's correctness with the summary output.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "    1. Fill in the input shape for the 28x28 image input.\n",
        "    2. Specify number of nodes for the dense layers.\n",
        "    3. Set the activation function for the hidden dense layers and ouput dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RkCjWdwKcBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "# define the input shape for a 28x28 grayscale image (like MNIST)\n",
        "# HINT: should be a tuple of height and width\n",
        "input_shape=( 28,28 )\n",
        "\n",
        "# Let's start a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Let's flatten the 28x28 image to a 784 1D vector\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "\n",
        "# Let's add the input layer as a dense layer of 128 nodes\n",
        "# HINT: the parameter is the number of nodes\n",
        "model.add(Dense(128))\n",
        "\n",
        "# Add the activation function\n",
        "# HINT: use the best practice convention for a non-output Dense layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the hidden layer with 512 nodes\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the output layer with 10 nodes\n",
        "model.add(Dense(10))\n",
        "\n",
        "# Add the activation function\n",
        "# HINT: use the best practice for a multi-class classifier\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45c6SLCKcBc",
        "colab_type": "text"
      },
      "source": [
        "### Verify the model architecture using summary method\n",
        "\n",
        "It should look like below:\n",
        "\n",
        "```\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "flatten_1 (Flatten)          (None, 784)               0         \n",
        "_________________________________________________________________\n",
        "dense_4 (Dense)              (None, 128)               100480    \n",
        "_________________________________________________________________\n",
        "activation_4 (Activation)    (None, 128)               0         \n",
        "_________________________________________________________________\n",
        "dense_5 (Dense)              (None, 512)               66048     \n",
        "_________________________________________________________________\n",
        "activation_5 (Activation)    (None, 512)               0         \n",
        "_________________________________________________________________\n",
        "dense_6 (Dense)              (None, 10)                5130      \n",
        "_________________________________________________________________\n",
        "activation_6 (Activation)    (None, 10)                0         \n",
        "=================================================================\n",
        "Total params: 171,658\n",
        "Trainable params: 171,658\n",
        "Non-trainable params: 0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrg7SKsnKcBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "4f0c0199-e2d2-4bbc-ebf0-eeb85c793390"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 171,658\n",
            "Trainable params: 171,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3dSG2YXKcBh",
        "colab_type": "text"
      },
      "source": [
        "##  Basic DNN as Sequential API (short form)\n",
        "\n",
        "Let's repeat the above, but use the short form.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "    1. Specify the activation functions as a parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9I94oudKcBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Let's start a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Let's add a first flattening layer to flatten the 28x28 image into 1D vector\n",
        "model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "\n",
        "# Let's add the input layer as a dense layer of 128 nodes\n",
        "# HINT: use best practices for activation functions\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add the hidden layer with 512 nodes\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Add the output layer with 10 nodes\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iSRgxNKcBm",
        "colab_type": "text"
      },
      "source": [
        "### Verify the model architecture using summary method\n",
        "\n",
        "It should be identical to the previous. But note in this case, summary() does not show the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D0Q7ZXkKcBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "39e8ba5d-c014-4739-d421-7a254872e13a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 171,658\n",
            "Trainable params: 171,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKPju8NSKcBr",
        "colab_type": "text"
      },
      "source": [
        "## Basic DNN as Sequential API (list form)\n",
        "\n",
        "Let's repeat the above, but instead specify the layers as a parameter to the Sequential() object, we will specify them using a list.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "    1. Define the hidden layer.\n",
        "    2. Define the output (classifier) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_XppU03KcBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Let's start a sequential model\n",
        "model = Sequential([ # input layer\n",
        "                     Flatten(input_shape=(28, 28, 1)),\n",
        "                     Dense(128, activation='relu'),\n",
        "                     # hidden layer\n",
        "                     Dense(512, activation='relu'),\n",
        "                     # output layer\n",
        "                     Dense(10, activation='softmax'),\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyJW82BxKcBv",
        "colab_type": "text"
      },
      "source": [
        "### Verify the model architecture using summary method\n",
        "\n",
        "It should be identical to the previous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIx56DwKcBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "8a969e4f-4d4c-45b8-ff62-b384e4c67b7e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 171,658\n",
            "Trainable params: 171,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bboLVYlSKcBz",
        "colab_type": "text"
      },
      "source": [
        "## Basic DNN as Functional API \n",
        "\n",
        "Let's repeat the above, but instead specify the layers using the functional API.\n",
        "\n",
        "This will need to:\n",
        "\n",
        "    1. Specify the input shape for the Input object\n",
        "    2. Connect the Input object to the Flatten layer.\n",
        "    3. Connect the Flatten output to the input Dense layer.\n",
        "    4. Specify the hidden and output layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk3GY8KcKcB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Define the input vector for the 28x28 images\n",
        "# HINT: Should be a tuple of height and width\n",
        "inputs = Input(shape=(28,28,1))\n",
        "\n",
        "# Let's flatten the image into a 1D vector\n",
        "# HINT: the input vector is where you defined the input shape\n",
        "x = Flatten()(inputs)\n",
        "\n",
        "# Define the input layer and connect the flattened input vector\n",
        "# HINT: the output of Flatten() is the flattened input vector\n",
        "x = Dense(128, activation='relu')(x)\n",
        "\n",
        "# Define the hidden layer and connect the hidden layer to it.\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "# Define the output layer and connect the hidden layer to it.\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Let's put it together: \n",
        "# HINT: inputs to outputs\n",
        "model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT2yI4vVKcB5",
        "colab_type": "text"
      },
      "source": [
        "### Verify the model architecture using summary method\n",
        "\n",
        "It should be identical to the previous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXLpYFcpKcB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "b29d6424-c3e3-407a-93f2-14c217d7bddb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 171,658\n",
            "Trainable params: 171,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5UQSUE_KcB_",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Finally, you will train the model --eventhough we haven't covered training yet. Just follow the steps and later we will revisit and go into details.\n",
        "\n",
        "We will make two copies of the model. One we will train without preprocessing (not covered yet) the MNIST dataset, and the other with preprocessing and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To3so3J9KcCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeModel():\n",
        "    # Make a DNN model\n",
        "    inputs = Input((28, 28))\n",
        "    x = Flatten()(inputs)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "# We make two copies of the model\n",
        "model_a = makeModel()\n",
        "model_b = makeModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUGJZ2ptKcCI",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "tf.Keras has a variety of builtin datasets, including the MNIST dataset. We will start by getting the dataset as non-processed data. This will include:\n",
        "\n",
        "    training data : *_train\n",
        "    test data .   : *_test\n",
        "    \n",
        "Both the training and test data consists of the training images (x_\\*) and corresponding labels (y_\\*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APbNr0jHKcCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "31baaeff-dbf1-4e41-9a70-79fc73b8c2b1"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6ySA0C2KcCN",
        "colab_type": "text"
      },
      "source": [
        "### Train w/o preprocessing the data\n",
        "\n",
        "Let's train the first model without preprocessing the data for 10 epochs (not covered yet). Pay attention to the end of the final (epoch 10) training accuracy (acc:) and validation accuracy (val_acc:)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQoBserKcCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "2307aefe-2949-47b1-b3db-6b492ad1634f"
      },
      "source": [
        "model_a.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 1.2735 - acc: 0.8502 - val_loss: 0.4832 - val_acc: 0.8965\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.4709 - acc: 0.8993 - val_loss: 0.3260 - val_acc: 0.9253\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.3645 - acc: 0.9169 - val_loss: 0.2480 - val_acc: 0.9405\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2367 - acc: 0.9413 - val_loss: 0.2096 - val_acc: 0.9443\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1730 - acc: 0.9525 - val_loss: 0.1568 - val_acc: 0.9580\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1516 - acc: 0.9581 - val_loss: 0.1448 - val_acc: 0.9607\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1384 - acc: 0.9623 - val_loss: 0.1552 - val_acc: 0.9593\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1281 - acc: 0.9653 - val_loss: 0.1606 - val_acc: 0.9608\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.1220 - acc: 0.9664 - val_loss: 0.1475 - val_acc: 0.9587\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.1096 - acc: 0.9705 - val_loss: 0.1586 - val_acc: 0.9623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe469734a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdAXTDXYKcCT",
        "colab_type": "text"
      },
      "source": [
        "### Training with preprocessed data.\n",
        "\n",
        "Let's preprocess the image data by normalizing the pixel values --not covered yet, and then train the second model. Pay attention to the final training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzjTytRKcCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "e1138042-add5-419e-ded6-440c68bce4cf"
      },
      "source": [
        "# Preprocess the image data\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "\n",
        "# Train the model\n",
        "model_b.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2286 - acc: 0.9305 - val_loss: 0.1179 - val_acc: 0.9645\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0985 - acc: 0.9689 - val_loss: 0.0938 - val_acc: 0.9720\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0696 - acc: 0.9777 - val_loss: 0.0811 - val_acc: 0.9763\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0531 - acc: 0.9827 - val_loss: 0.0900 - val_acc: 0.9750\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.0812 - val_acc: 0.9797\n",
            "Epoch 6/10\n",
            " 159/1688 [=>............................] - ETA: 5s - loss: 0.0315 - acc: 0.9912"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWBxHfawKcCY",
        "colab_type": "text"
      },
      "source": [
        "### Observation\n",
        "\n",
        "Note how by preprocessing the data, we get a higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQW-4-KZKcCZ",
        "colab_type": "text"
      },
      "source": [
        "## End of Lab"
      ]
    }
  ]
}